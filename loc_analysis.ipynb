{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "hybridized_code_path = os.getcwd() + \"/asuca_sample/hybrid\"\n",
    "reference_code_path = os.getcwd() + \"/asuca_sample/reference\"\n",
    "\n",
    "# full ASUCA source directory (cannot be published at this time)\n",
    "# hybridized_code_path = \"/Users/muellermichel/typhoon/Repository/asuca/branches/hybrid/asuca-kij\"\n",
    "# reference_code_path = \"/Users/muellermichel/typhoon/Repository/asuca/branches/reference/asuca-kij\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Sample directories\n",
    "hybridized_code_directories = [hybridized_code_path]\n",
    "reference_code_directories = [reference_code_path]\n",
    "\n",
    "# Uncomment this for the full ASUCA source\n",
    "# hybridized_code_directories = [\n",
    "#     hybridized_code_path + \"/HybridSources\",\n",
    "#     hybridized_code_path + \"/Framework\"\n",
    "# ]\n",
    "# reference_code_directories = [\n",
    "#     reference_code_path + \"/Src\",\n",
    "#     reference_code_path + \"/Tools\"\n",
    "# ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re, os\n",
    "\n",
    "def lines_from_file(path):\n",
    "    with open(path) as f:\n",
    "        return f.readlines()\n",
    "    return None\n",
    "\n",
    "def areIndexesWithinQuotes(stringToSearch):\n",
    "    #build up a colored list, showing whether an index inside stringToSearch is in quotes or not.\n",
    "    #nested quotes such as \"hello'world' foobar\" are not supported!\n",
    "    quoteSections = re.split(r'''(['\"])''', stringToSearch)\n",
    "    isStringIndexWithinQuote = isStringIndexWithinQuote = [False] * len(stringToSearch)\n",
    "    if len(quoteSections) < 2:\n",
    "        pass\n",
    "    elif (len(quoteSections) - 1) % 2 != 0:\n",
    "        raise Exception(\"Unexpected behavior of regex split. Please check your python version.\")\n",
    "    elif (len(quoteSections) - 1) % 4 != 0: #check re.split documentation to see how it works.\n",
    "        pass\n",
    "    else:\n",
    "        quoteSections.reverse()\n",
    "        currSection = quoteSections.pop()\n",
    "        index = len(currSection)\n",
    "        if index > 0:\n",
    "            isStringIndexWithinQuote[0:index] = [False] * len(currSection)\n",
    "        while len(quoteSections) > 0:\n",
    "            #opening quote part\n",
    "            currSection = quoteSections.pop()\n",
    "            sectionLength = len(currSection)\n",
    "            prefIndex = 0\n",
    "            if index > 0:\n",
    "                prefIndex = index\n",
    "            index = index + sectionLength\n",
    "            if sectionLength != 1:\n",
    "                raise Exception(\"Quote begin marker with strange number of characters\")\n",
    "            isStringIndexWithinQuote[prefIndex:index] = [True]\n",
    "\n",
    "            #inbetween quotes part\n",
    "            currSection = quoteSections.pop()\n",
    "            sectionLength = len(currSection)\n",
    "            prefIndex = index\n",
    "            index = index + sectionLength\n",
    "            isStringIndexWithinQuote[prefIndex:index] = [True] * sectionLength\n",
    "\n",
    "            #closing quote part\n",
    "            currSection = quoteSections.pop()\n",
    "            sectionLength = len(currSection)\n",
    "            prefIndex = index\n",
    "            index = index + sectionLength\n",
    "            if sectionLength != 1:\n",
    "                raise Exception(\"Quote end marker with strange number of characters\")\n",
    "            isStringIndexWithinQuote[prefIndex:index] = [True]\n",
    "\n",
    "            #next part that's not within quotes\n",
    "            currSection = quoteSections.pop()\n",
    "            sectionLength = len(currSection)\n",
    "            prefIndex = index\n",
    "            index = index + sectionLength\n",
    "            isStringIndexWithinQuote[prefIndex:index] = [False] * sectionLength\n",
    "        #sanity check\n",
    "        if index != len(stringToSearch):\n",
    "            raise Exception(\"Index at the end of quotes search is %i. Expected: %i\" %(index, len(stringToSearch)))\n",
    "    return isStringIndexWithinQuote\n",
    "\n",
    "def findLeftMostOccurrenceNotInsideQuotes(stringToMatch, stringToSearch, leftStartAt=-1, filterOutEmbeddings=False):\n",
    "    indexesWithinQuotes = areIndexesWithinQuotes(stringToSearch)\n",
    "    nextLeftStart = leftStartAt + 1\n",
    "    matchIndex = -1\n",
    "    for numOfTrys in range(1,101):\n",
    "        if nextLeftStart >= len(stringToSearch):\n",
    "            break\n",
    "        currSlice = stringToSearch[nextLeftStart:]\n",
    "        matchIndex = currSlice.find(stringToMatch)\n",
    "        if matchIndex < 0:\n",
    "            break\n",
    "        matchEndIndex = matchIndex + len(stringToMatch)\n",
    "        if not indexesWithinQuotes[nextLeftStart:][matchIndex] \\\n",
    "        and (not filterOutEmbeddings or nextLeftStart + matchIndex < 1 or re.match(r'\\W', stringToSearch[nextLeftStart + matchIndex - 1])) \\\n",
    "        and (not filterOutEmbeddings or len(stringToSearch) <= nextLeftStart + matchEndIndex or re.match(r'\\W', stringToSearch[nextLeftStart + matchEndIndex])):\n",
    "            break\n",
    "        nextLeftStart += matchIndex + 1\n",
    "        matchIndex = -1\n",
    "        if numOfTrys >= 100:\n",
    "            raise Exception(\"Could not find the string even after 100 tries.\")\n",
    "    return matchIndex + nextLeftStart if matchIndex >= 0 else -1\n",
    "\n",
    "openMPLinePattern = re.compile(r'\\s*\\!\\$OMP.*', re.IGNORECASE)\n",
    "openACCLinePattern = re.compile(r'\\s*\\!\\$ACC.*', re.IGNORECASE)\n",
    "pgiPragmaLinePattern = re.compile(r'\\s*\\!PGI.*', re.IGNORECASE)\n",
    "\n",
    "def strip_and_filter_comments(lines):\n",
    "    filtered = []\n",
    "    for line in lines:\n",
    "        if len(line.strip()) == 0:\n",
    "            continue\n",
    "        if openMPLinePattern.match(line) or openACCLinePattern.match(line) or pgiPragmaLinePattern.match(line):\n",
    "            filtered.append(line.strip())\n",
    "            continue\n",
    "        commentIndex = findLeftMostOccurrenceNotInsideQuotes(\"!\", line)\n",
    "        if commentIndex < 0:\n",
    "            filtered.append(line.strip())\n",
    "            continue\n",
    "        if len(line[:commentIndex].strip()) > 0:\n",
    "            filtered.append(line[:commentIndex].strip())\n",
    "    return filtered\n",
    "\n",
    "openMPLinePattern = re.compile(r'\\s*\\!\\$OMP.*', re.IGNORECASE)\n",
    "openACCLinePattern = re.compile(r'\\s*\\!\\$ACC.*', re.IGNORECASE)\n",
    "pgiPragmaLinePattern = re.compile(r'\\s*\\!PGI.*', re.IGNORECASE)\n",
    "commentedPattern = re.compile(r'^\\s*!\\s*(.*)', re.IGNORECASE)\n",
    "emptyLinePattern = re.compile(r'(.*?)(?:[\\n\\r\\f\\v]+[ \\t]*)+(.*)', re.DOTALL)\n",
    "multiLineContinuationPattern = re.compile(r'(.*?)\\s*\\&\\s+(?:\\!?\\$?(?:OMP|ACC)?\\&?)?\\s*(.*)', re.DOTALL | re.IGNORECASE)\n",
    "\n",
    "def strip_combine_and_remove_comments(file_obj, keep_comments=False):\n",
    "    #first pass: strip out commented code (otherwise we could get in trouble when removing line continuations, if there are comments in between)\n",
    "    remainder = \"\"\n",
    "    if not keep_comments:\n",
    "        noComments = \"\"\n",
    "        for line in file_obj:\n",
    "            if openMPLinePattern.match(line) or openACCLinePattern.match(line) or pgiPragmaLinePattern.match(line):\n",
    "                noComments += line\n",
    "                continue\n",
    "            commentIndex = findLeftMostOccurrenceNotInsideQuotes(\"!\", line)\n",
    "            if commentIndex < 0:\n",
    "                noComments += line\n",
    "                continue\n",
    "            noComments += line[:commentIndex] + \"\\n\"\n",
    "        remainder = noComments\n",
    "    else:\n",
    "        for line in file_obj:\n",
    "            commented_match = commentedPattern.match(line)\n",
    "            if commented_match:\n",
    "                remainder += (\"!%s\" %(commented_match.group(1))).strip() + \"\\n\"\n",
    "                continue\n",
    "            elif openMPLinePattern.match(line) or openACCLinePattern.match(line) or pgiPragmaLinePattern.match(line):\n",
    "                remainder += line\n",
    "                continue\n",
    "                \n",
    "            commentIndex = findLeftMostOccurrenceNotInsideQuotes(\"!\", line.strip())\n",
    "            if commentIndex < 0:\n",
    "                remainder += line.strip() + \"\\n\"\n",
    "            else:\n",
    "                remainder += line.strip()[:commentIndex] + \"\\n\"\n",
    "    \n",
    "    #second pass: strip out empty lines (otherwise we could get in trouble when removing line continuations, if there are empty lines in between)\n",
    "    stripped = \"\" \n",
    "    while True:\n",
    "        emptyLineMatch = emptyLinePattern.match(remainder)\n",
    "        if not emptyLineMatch:\n",
    "            stripped += remainder\n",
    "            break\n",
    "        stripped += emptyLineMatch.group(1).strip() + \"\\n\"\n",
    "        remainder = emptyLineMatch.group(2)\n",
    "\n",
    "    #third pass: remove line continuations\n",
    "    remainder = stripped\n",
    "    output = \"\"\n",
    "    while True:\n",
    "        lineContinuationMatch = multiLineContinuationPattern.match(remainder)\n",
    "        if not lineContinuationMatch:\n",
    "            output += remainder\n",
    "            break\n",
    "        output += lineContinuationMatch.group(1) + \" \"\n",
    "        remainder = lineContinuationMatch.group(2)\n",
    "\n",
    "    return (output, stripped)\n",
    "\n",
    "def sanitized_lines_from_file(path, keep_comments=False):\n",
    "    with open(path) as f:\n",
    "        combined, stripped_only = strip_combine_and_remove_comments(f, keep_comments)\n",
    "        return (combined.split(\"\\n\")[:-1], stripped_only.split(\"\\n\")[:-1])\n",
    "    return (None, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Source Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pattern Matching and Counter Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import namedtuple\n",
    "\n",
    "Analysis = namedtuple(\n",
    "    \"Analysis\",\n",
    "    \"stripped, combined, changed, unchanged, subroutine_definitions, calls, imports, data_definitions, save, parameter, openmp, style, parallel_loops, device_data_init, implementation_scheme, commented, radiation, storage_order, data_spec, other\"\n",
    ")\n",
    "\n",
    "def analyze_source(ref_source_path, hf_source_path):\n",
    "    ref_lines, ref_with_cont = sanitized_lines_from_file(ref_source_path)\n",
    "    hf_lines_set = set(sanitized_lines_from_file(hf_source_path, keep_comments=True)[0]) if hf_source_path else set()\n",
    "    changed_lines = [l for l in ref_lines if l not in hf_lines_set]\n",
    "    unchanged_lines = [l for l in ref_lines if l in hf_lines_set]\n",
    "    \n",
    "    subroutine_definitions = []\n",
    "    calls = []\n",
    "    imports = []\n",
    "    data_definitions = []\n",
    "    save = []\n",
    "    parameter = []\n",
    "    style = []\n",
    "    openmp = []\n",
    "    parallel_loops = []\n",
    "    device_data_init = []\n",
    "    implementation_scheme = []\n",
    "    commented = []\n",
    "    radiation = []\n",
    "    storage_order = []\n",
    "    data_spec = []\n",
    "    other = []\n",
    "    for c in changed_lines:\n",
    "        if \"storage_order\" in ref_source_path:\n",
    "            storage_order.append(c)\n",
    "        elif \"!%s\" %(c) in hf_lines_set:\n",
    "            commented.append(c)\n",
    "        elif re.match(r'(^|\\W)subroutine\\s+', c, re.IGNORECASE):\n",
    "            subroutine_definitions.append(c)\n",
    "        elif re.match(r'(^|\\W)use\\s+', c, re.IGNORECASE):\n",
    "            imports.append(c)\n",
    "        elif re.match(r'(.*?\\(\\/.*|^data)', c, re.IGNORECASE):\n",
    "            data_definitions.append(c)\n",
    "        elif re.match(r'.*?\\Wsave\\W', c, re.IGNORECASE):\n",
    "            save.append(c)\n",
    "        elif re.match(r'.*?\\parameter\\W', c, re.IGNORECASE):\n",
    "            parameter.append(c)\n",
    "        elif re.match(r'(^return$|end\\s+subroutine\\s+\\w+|^write\\(.*)', c, re.IGNORECASE):\n",
    "            style.append(c) #JMA often uses superfluous return before subroutine end - we often removed this\n",
    "            #we also often removed repetition of subroutine name in end statement\n",
    "            #we also changed the output stream from 6 to 0\n",
    "        elif re.match(r'^\\s*\\!\\s*\\$.*', c, re.IGNORECASE):\n",
    "            openmp.append(c)\n",
    "        elif re.match(r'^do\\s+[ij].*', c, re.IGNORECASE):\n",
    "            parallel_loops.append(c)\n",
    "        elif re.match(r'.*?_hfdev.*|.*?@if.*|.*?@end\\s+if.*', c, re.IGNORECASE):\n",
    "            device_data_init.append(c)\n",
    "        elif re.match(r'.*?@scheme.*|.*?@end scheme.*', c, re.IGNORECASE):\n",
    "            implementation_scheme.append(c)\n",
    "        elif re.match(r'^call\\s.*', c, re.IGNORECASE):\n",
    "            calls.append(c)\n",
    "        elif \"rad_\" in ref_source_path and not re.match(r'^\\s*@.*|^[\\s\\w,]*$', c, re.IGNORECASE):\n",
    "            radiation.append(c)\n",
    "        elif re.match(r'(^real\\W|^(de)?allocate\\W)|(:(,)?)+\\)\\s*\\=\\s*0\\.0', c, re.IGNORECASE):\n",
    "            data_spec.append(c)\n",
    "        else:\n",
    "            other.append(c)\n",
    "    print(\"\\n\".join(other))\n",
    "    \n",
    "    return Analysis(\n",
    "        stripped= len(ref_with_cont),\n",
    "        combined= len(ref_lines),\n",
    "        changed= len(changed_lines),\n",
    "        unchanged= len(unchanged_lines),\n",
    "        subroutine_definitions= len(subroutine_definitions),\n",
    "        calls= len(calls),\n",
    "        imports = len(imports),\n",
    "        data_definitions= len(data_definitions),\n",
    "        save= len(save),\n",
    "        parameter= len(parameter),\n",
    "        openmp= len(openmp),\n",
    "        style = len(style),\n",
    "        parallel_loops= len(parallel_loops),\n",
    "        device_data_init= len(device_data_init),\n",
    "        implementation_scheme= len(implementation_scheme),\n",
    "        commented= len(commented),\n",
    "        radiation= len(radiation),\n",
    "        storage_order= len(storage_order),\n",
    "        data_spec= len(data_spec),\n",
    "        other= len(other)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of Entire Source Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def fortran_sources_list(directory_paths):\n",
    "    import os\n",
    "    return [\n",
    "        os.path.join(dp, f)\n",
    "        for path in directory_paths\n",
    "        for dp, dn, filenames in os.walk(path)\n",
    "        for f in filenames if os.path.splitext(f)[1] in ['.f90', '.F90', '.h90', '.H90']\n",
    "    ]\n",
    "\n",
    "ignore_list = set([\n",
    "    \"mp_nhm_option_symbol\", \"pbl_mym_phi\",\n",
    "    \"pbl_sib_coupler\", \"main_gabls3\", \"main_pp\"\n",
    "])\n",
    "alias_by_basename = {\n",
    "    \"rad_jma0507\":\"rad_jma\"\n",
    "}\n",
    "ref_alias_by_basename = {\n",
    "    \"rad_jma\":\"rad_jma0507\"\n",
    "}\n",
    "\n",
    "def analyze_sources(source_directory_paths, comparison_source_directory_paths):\n",
    "    import os\n",
    "    comparison_sources_by_basename = {\n",
    "        os.path.splitext(os.path.basename(path))[0]:path\n",
    "        for path in fortran_sources_list(comparison_source_directory_paths)\n",
    "    }\n",
    "    sources = fortran_sources_list(source_directory_paths)\n",
    "\n",
    "    # sanitized_lines_from_file(comparison_sources_by_basename[\"ideal\"], keep_comments=True)\n",
    "\n",
    "    # [l for l in sanitized_lines_from_file(comparison_sources_by_basename[\"ideal\"], keep_comments=True)[0] if \"read(10)\" in l]\n",
    "\n",
    "    # print Analysis(*[sum(x) for x in zip(\n",
    "    #     analyze_source(sources[0], comparison_sources_by_basename[\"adv\"]),\n",
    "    #     analyze_source(sources[1], comparison_sources_by_basename[\"coriolis\"])\n",
    "    # )])\n",
    "\n",
    "    analysis_list = []\n",
    "    for r in sources:\n",
    "        basename = os.path.splitext(os.path.basename(r))[0]\n",
    "        basename = alias_by_basename.get(basename, basename)\n",
    "        hf_source = comparison_sources_by_basename.get(basename)\n",
    "        if basename not in ignore_list and not \"sib_\" in basename:\n",
    "            print \"========== \" + basename + \" =============\"\n",
    "            print \"refpath: %s, hfpath: %s\" %(r, hf_source)\n",
    "            analysis = analyze_source(r, hf_source)\n",
    "            print analyze_source(r, hf_source)\n",
    "            analysis_list.append(analysis)\n",
    "    print \"========== TOTAL =============\"\n",
    "    print Analysis(*[sum(x) for x in zip(*analysis_list)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output Generation for Comparison Reference vs. Hybrid\n",
    "generates \"output_reference_vs_hybrid.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== lbc =============\n",
      "refpath: /Users/muellermichel/Dropbox/Apps/ShareLaTeX/DirectivesWorkshopPaper/data/productivity_analysis/asuca_sample/reference/lbc.f90, hfpath: /Users/muellermichel/Dropbox/Apps/ShareLaTeX/DirectivesWorkshopPaper/data/productivity_analysis/asuca_sample/hybrid/lbc.h90\n",
      "\n",
      "\n",
      "Analysis(stripped=38, combined=30, changed=7, unchanged=23, subroutine_definitions=1, calls=0, imports=2, data_definitions=0, save=0, parameter=0, openmp=2, style=0, parallel_loops=2, device_data_init=0, implementation_scheme=0, commented=0, radiation=0, storage_order=0, data_spec=0, other=0)\n",
      "========== surface =============\n",
      "refpath: /Users/muellermichel/Dropbox/Apps/ShareLaTeX/DirectivesWorkshopPaper/data/productivity_analysis/asuca_sample/reference/surface.f90, hfpath: /Users/muellermichel/Dropbox/Apps/ShareLaTeX/DirectivesWorkshopPaper/data/productivity_analysis/asuca_sample/hybrid/surface.h90\n",
      "\n",
      "\n",
      "Analysis(stripped=114, combined=86, changed=4, unchanged=82, subroutine_definitions=0, calls=3, imports=0, data_definitions=0, save=0, parameter=0, openmp=0, style=1, parallel_loops=0, device_data_init=0, implementation_scheme=0, commented=0, radiation=0, storage_order=0, data_spec=0, other=0)\n",
      "========== TOTAL =============\n",
      "Analysis(stripped=152, combined=116, changed=11, unchanged=105, subroutine_definitions=1, calls=3, imports=2, data_definitions=0, save=0, parameter=0, openmp=2, style=1, parallel_loops=2, device_data_init=0, implementation_scheme=0, commented=0, radiation=0, storage_order=0, data_spec=0, other=0)\n"
     ]
    }
   ],
   "source": [
    "analyze_sources(reference_code_directories, hybridized_code_directories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output Generation for Comparison Hybrid vs. Reference\n",
    "generates \"output_hybrid_vs_reference.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== lbc =============\n",
      "refpath: /Users/muellermichel/Dropbox/Apps/ShareLaTeX/DirectivesWorkshopPaper/data/productivity_analysis/asuca_sample/hybrid/lbc.h90, hfpath: /Users/muellermichel/Dropbox/Apps/ShareLaTeX/DirectivesWorkshopPaper/data/productivity_analysis/asuca_sample/reference/lbc.f90\n",
      "@domainDependant{attribute(autoDom, present), accPP(AT_TIGHT_STENCIL), domPP(DOM_TIGHT_STENCIL)}\n",
      "dens_ref_f, dens_ptb_damp\n",
      "@end domainDependant\n",
      "@domainDependant{attribute(autoDom, present), accPP(AT4_TIGHT_STENCIL), domPP(DOM4_TIGHT_STENCIL)}\n",
      "dens_ptb_bnd\n",
      "@end domainDependant\n",
      "@parallelRegion{domName(i,j), domSize(nx_mn:nx_mx,ny_mn:ny_mx), startAt(nx_mn,ny_mn), endAt(nx_mx,ny_mx), template(TIGHT_STENCIL)}\n",
      "@end parallelRegion\n",
      "@domainDependant{attribute(autoDom, present), accPP(AT_TIGHT_STENCIL), domPP(DOM_TIGHT_STENCIL)}\n",
      "dens_ref_f, dens_ptb_damp\n",
      "@end domainDependant\n",
      "@domainDependant{attribute(autoDom, present), accPP(AT4_TIGHT_STENCIL), domPP(DOM4_TIGHT_STENCIL)}\n",
      "dens_ptb_bnd\n",
      "@end domainDependant\n",
      "@parallelRegion{domName(i,j), domSize(nx_mn:nx_mx,ny_mn:ny_mx), startAt(nx_mn,ny_mn), endAt(nx_mx,ny_mx), template(TIGHT_STENCIL)}\n",
      "@end parallelRegion\n",
      "Analysis(stripped=39, combined=31, changed=10, unchanged=21, subroutine_definitions=1, calls=0, imports=1, data_definitions=0, save=0, parameter=0, openmp=0, style=0, parallel_loops=0, device_data_init=0, implementation_scheme=0, commented=0, radiation=0, storage_order=0, data_spec=0, other=8)\n",
      "========== surface =============\n",
      "refpath: /Users/muellermichel/Dropbox/Apps/ShareLaTeX/DirectivesWorkshopPaper/data/productivity_analysis/asuca_sample/hybrid/surface.h90, hfpath: /Users/muellermichel/Dropbox/Apps/ShareLaTeX/DirectivesWorkshopPaper/data/productivity_analysis/asuca_sample/reference/surface.f90\n",
      "@domainDependant{domName(i, j), domSize(nx, ny), attribute(autoDom, present)}\n",
      "u1, v1, pt1, qv1, r_exner_surf, z_f1,\n",
      "skind, beta, qsatg, solar, tlcvr,\n",
      "z_0m, z_0h, z_0q, r_mol, u_f,\n",
      "taux_surf_ex, tauy_surf_ex, ftl_surf_ex, fqw_surf_ex, r_mol_surf, u_f_surf,\n",
      "taux_tile_ex, tauy_tile_ex, ftl_tile_ex, fqw_tile_ex, dfm_tile, dfh_tile, dfq_tile\n",
      "@end domainDependant\n",
      "@domainDependant{attribute(present), domName(it1,it2,i,j), domSize(0:ngm,ntlm,0:nx+1,0:ny+1)}\n",
      "tg\n",
      "@end domainDependant\n",
      "@parallelRegion{appliesTo(GPU), domName(i,j), domSize(nx,ny)}\n",
      "@end parallelRegion\n",
      "@domainDependant{domName(i, j), domSize(nx, ny), attribute(autoDom, present)}\n",
      "u1, v1, pt1, qv1, r_exner_surf, z_f1,\n",
      "skind, beta, qsatg, solar, tlcvr,\n",
      "z_0m, z_0h, z_0q, r_mol, u_f,\n",
      "taux_surf_ex, tauy_surf_ex, ftl_surf_ex, fqw_surf_ex, r_mol_surf, u_f_surf,\n",
      "taux_tile_ex, tauy_tile_ex, ftl_tile_ex, fqw_tile_ex, dfm_tile, dfh_tile, dfq_tile\n",
      "@end domainDependant\n",
      "@domainDependant{attribute(present), domName(it1,it2,i,j), domSize(0:ngm,ntlm,0:nx+1,0:ny+1)}\n",
      "tg\n",
      "@end domainDependant\n",
      "@parallelRegion{appliesTo(GPU), domName(i,j), domSize(nx,ny)}\n",
      "@end parallelRegion\n",
      "Analysis(stripped=132, combined=101, changed=19, unchanged=82, subroutine_definitions=0, calls=3, imports=4, data_definitions=0, save=0, parameter=0, openmp=0, style=0, parallel_loops=0, device_data_init=0, implementation_scheme=0, commented=0, radiation=0, storage_order=0, data_spec=0, other=12)\n",
      "========== TOTAL =============\n",
      "Analysis(stripped=171, combined=132, changed=29, unchanged=103, subroutine_definitions=1, calls=3, imports=5, data_definitions=0, save=0, parameter=0, openmp=0, style=0, parallel_loops=0, device_data_init=0, implementation_scheme=0, commented=0, radiation=0, storage_order=0, data_spec=0, other=20)\n"
     ]
    }
   ],
   "source": [
    "analyze_sources(hybridized_code_directories, reference_code_directories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding Routines with Changed Kernel Positions\n",
    "Requires cpu_routine_positions.json and gpu_routine_positions.json - these were generated using the Hybrid Fortran toolchain applied to the Hybrid ASUCA codebase.\n",
    "\n",
    "This gives the physics routine names that require a granularity change between CPU and GPU, and thus what needs to be reimplmenented separately in an OpenACC implementation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/muellermichel/Dropbox/Apps/ShareLaTeX/DirectivesWorkshopPaper/data/productivity_analysis\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "215"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "print os.getcwd()\n",
    "\n",
    "cpu_positions_by_name = {}\n",
    "with open(os.getcwd() + \"/cpu_routine_positions.json\") as f:\n",
    "    cpu_positions_by_name = json.loads(f.read())\n",
    "\n",
    "gpu_positions_by_name = {}\n",
    "with open(os.getcwd() + \"/gpu_routine_positions.json\") as f:\n",
    "    gpu_positions_by_name = json.loads(f.read())\n",
    "\n",
    "routines_with_changed_position = []\n",
    "for routine, position in gpu_positions_by_name.items():\n",
    "    if routine not in cpu_positions_by_name:\n",
    "        raise Exception(routine + \" not found.\")\n",
    "    if position != cpu_positions_by_name[routine]:\n",
    "#         print routine, position, cpu_positions_by_name[routine]\n",
    "        routines_with_changed_position.append(routine)\n",
    "len(routines_with_changed_position)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding the LOC for Routines with Changed Kernel Positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "from collections import namedtuple\n",
    "\n",
    "State = namedtuple(\"State\", \"subroutine_name, start_line_no\")\n",
    "\n",
    "def scan_loc_by_routine(source_path):\n",
    "    lines, _ = sanitized_lines_from_file(source_path)\n",
    "    state = None\n",
    "    result = {}\n",
    "    for line_no, line in enumerate(lines):\n",
    "        if not state:\n",
    "            subroutine_match = re.match('^\\s*subroutine\\s+(\\w*).*', line, re.IGNORECASE)\n",
    "            if subroutine_match:\n",
    "                state = State(subroutine_match.group(1), line_no)\n",
    "        else:\n",
    "            end_subroutine_match = re.match('^\\s*end\\s*subroutine.*', line, re.IGNORECASE)\n",
    "            if end_subroutine_match:\n",
    "                result[state.subroutine_name] = line_no - state.start_line_no\n",
    "                state = None\n",
    "    if state:\n",
    "        raise Exception(\"unfinished routine %s in %s\" %(state.subroutine_name, source_path))\n",
    "    return result\n",
    "\n",
    "loc_by_routine = {}\n",
    "for source in fortran_sources_list(reference_code_directories):\n",
    "    loc_by_routine.update(scan_loc_by_routine(source))\n",
    "\n",
    "loc = 0\n",
    "for routine in routines_with_changed_position:\n",
    "    alias = ref_alias_by_basename.get(routine, routine)\n",
    "    if alias in loc_by_routine:\n",
    "        loc += loc_by_routine[alias]\n",
    "loc"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
